{"cells":[{"cell_type":"markdown","source":["<img src=\"https://www.mercari.com/assets/img/help_center/us/ogp.png\"/>\n\n# Mercari Price Suggestion Challenge\n***\n### Can you automatically suggest product prices to online sellers?\n\n**Product pricing gets even harder at scale**, considering just how many products are sold online. Clothing has strong seasonal pricing trends and is heavily influenced by brand names, while electronics have fluctuating prices based on product specs.\n\n**Mercari**, Japan’s biggest community-powered shopping app, knows this problem deeply. They’d like to offer pricing suggestions to sellers, but this is tough because their sellers are enabled to put just about anything, or any bundle of things, on Mercari's marketplace.\n\nIn this competition, Mercari’s challenging you to **build an algorithm that automatically suggests the right product prices**. You’ll be provided user-inputted text descriptions of their products, including details like product category name, brand name, and item condition.\n\n### Dataset Features\n\n- **ID**: the id of the listing\n- **Name:** the title of the listing\n- **Item Condition:** the condition of the items provided by the seller\n- **Category Name:** category of the listing\n- **Brand Name:** brand of the listing\n- **Shipping:** whether or not shipping cost was provided\n- **Item Description:** the full description of the item\n- **Price:** the price that the item was sold for. This is the target variable that you will predict. The unit is USD.\n\n**Work on supply and demand**\n\n**Source:** https://www.kaggle.com/c/mercari-price-suggestion-challenge"],"metadata":{}},{"cell_type":"markdown","source":["<img src = \"https://cdn.dribbble.com/users/56196/screenshots/2281553/mobile-dribbble.gif\"/>"],"metadata":{}},{"cell_type":"markdown","source":["# Representing and Mining Text\n***\nSince, text is the most **unstructured** form of all the available data, various types of noise are present in it and the data is not readily analyzable without any pre-processing. The entire process of cleaning and standardization of text, making it noise-free and ready for analysis is known as **text pre-processing**.\n\n### Fundamental Concepts \n\nThe importance of constructing mining-friendly data representations; Representation of text for data mining. \n\n### Important Terminologies\n- **Document**: One piece of text. It could be a single sentence, a paragraph, or even a full page report. \n- **Tokens**: Also known as terms. It is simply just a word. So many tokens form a document. \n- **Corpus**: A collection of documents. \n- **Term Frequency (TF)**: Measures how often a term is in a single document\n- **Inverse Document Frequency (IDF)**: distribution of a term over a corpus\n\n### Pre-Processing Techniques\n- **Stop Word Removal:** stop words are terms that have little no meaning in a given text. Think of it as the \"noise\" of data. Such terms include the words, \"the\", \"a\", \"an\", \"to\", and etc...\n- **Bag of Words Representation: ** treats each word as a feature of the document\n\n- **TFIDF**: a common value representation of terms. It boosts or weighs words that have low occurences. For example, if the word \"play\" is common, then there is little to no boost. But if the word \"mercari\" is rare, then it has more boosts/weight. \n\n- **N-grams**: Sequences of adjacent words as terms. For example, since a word by itself may have little to no value, but if you were to put two words together and analyze it as a pair, then it might add more meaning. For example, \"iPhone\" VS \"iPhone Charger\"\n\n- **Stemming and Lemmatization**: Get the root meaning of the word\n\n- **Topic Models**: A type of model that represents a set of topics from a sequence of words."],"metadata":{}},{"cell_type":"markdown","source":["# Table of Content\n***\n### Define the Problem:\n\n- [What's the Business Goal?](#map_of_newyork)\n\n- [How will the solution be used?](#asian_white_geomap)\n\n- [How to frame the problem?](#black_hispanic_geomap)\n\n- [What metric are we optimizing?](#black_hispanic_geomap)\n\n### Descriptive Statistics:\n- [Observe Training Statistics](#correlation)\n\n- [Simple Data Inspection](#correlation)\n\n- [Missing Value Treatment](#correlation)\n\n\n### Exploratory Data Analysis:\n- [Price Distribution & Log Transformation](#correlation)\n\n- [Shipping Type Distribution](#race_economic)\n\n- [Category Distribution & Feature Engineering](#school_attendance)\n\n- [Brand Analysis](#student_performance)\n\n- [Length of Item Description VS Price](#math_test)\n\n\n### Text Processing:\n- [Normalizing Words (Stemming, Lowercase, Punctuation, Stop Words](#community_vs_noncommunity)\n\n### Feature Extraction with Text:\n- [Bag of Words Model](#community_vs_noncommunity)\n\n- [Word Tokens](#economic_need)\n\n- [Word Frequency Weights](#avg_school_income_comparison)\n\n- [CountVectorizer, TF-IDF, LabelBinarizer](#avg_school_income_comparison)\n\n- [Encoding Categorical Variables](#avg_school_income_comparison)\n\n\n# Define the Problem\n\n**A. Define the objective in business terms:** The objective is to come up with the right pricing algorithm that can we can use as a pricing recommendation to the users. \n\n**B. How will your solution be used?:** Allowing the users to see a suggest price before purchasing or selling will hopefully allow more transaction within Mercari's business. \n\n**C. How should you frame this problem?:** This problem can be solved using a supervised learning approach, and possible some unsupervised learning methods as well for clustering analysis. \n\n**D. How should performance be measured?:** Since its a regression problem, the evaluation metric that should be used is RMSE (Root Mean Squared Error). But in this case for the competition, we'll be using the RMSLE; which puts less penalty on large errors and focuses more on the smaller errors (since our main distribution in price is centered at around $10)\n\n** E. Are there any other data sets that you could use?:** To get a more accurate understanding and prediction for this problem, a potential dataset that we can gather would be more about the user. Features such as user location, user gender, and seasonality."],"metadata":{}},{"cell_type":"markdown","source":["# Import Packages"],"metadata":{}},{"cell_type":"code","source":["__author__ = \"Data Science Dream Job\"\n__copyright__ = \"Copyright 2018, Data Science Dream Job LLC\"\n__email__ = \"info@datasciencedreamjob.com\""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["import time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import LabelBinarizer"],"metadata":{"collapsed":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Training Set\n\nThe training set has about 600,000 observations"],"metadata":{}},{"cell_type":"code","source":["# Observe the training set\ntrain = pd.read_csv('C:/Users/Randy/Desktop/training/train.tsv', sep = '\\t')\ntrain.head()"],"metadata":{"scrolled":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":["print(\"The size of the training data is: \" + str(train.shape))\nprint(train.dtypes)"],"metadata":{"scrolled":false},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## Summary Statistics:\n- Most item price are at 10 Dollars\n- There are about 33k items with no descriptions\n- There are 3751 unique brands\n- Majority of the items are Women's Brand"],"metadata":{}},{"cell_type":"code","source":["train.astype('object').describe().transpose()"],"metadata":{"scrolled":false},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["## Test Set\nThe testing set has about 700,000 observations"],"metadata":{}},{"cell_type":"code","source":["# Observe testing set\ntest = pd.read_csv('C:/Users/Randy/Desktop/training/test.tsv', sep = '\\t',engine = 'python')\ntest.head()"],"metadata":{"scrolled":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":["test.shape"],"metadata":{"scrolled":true},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["# Reduce Training Size\nSample only 10% of the training set for now... to save time"],"metadata":{}},{"cell_type":"code","source":["reduced_train = train.sample(frac=0.3).reset_index(drop=True)\nreduced_train.shape"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["# Fast Data Cleaning"],"metadata":{}},{"cell_type":"code","source":["# How many missing values do we have in our training set\n# Why is brand name missing? Do all items need a brand? \ntrain.isnull().sum()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# Create a function to impute missing values\ndef fill_missing_value(data):\n    data['category_name'].fillna(value = 'Other', inplace=True)\n    data['brand_name'].fillna(value = 'uknown', inplace=True)\n    data['item_description'].fillna(value = 'No description yet', inplace=True)\n    \n    return data"],"metadata":{"collapsed":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":["train = fill_missing_value(train)\ntrain.isnull().sum()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["# Exploratory Data Analysis"],"metadata":{}},{"cell_type":"markdown","source":["### Examine Target Value (Price)"],"metadata":{}},{"cell_type":"code","source":["train.price.describe()"],"metadata":{"scrolled":true},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["# Price Distribution\n**Summary:**\n- The mean price in the dataset is **26 Dollars**\n- The median price in the dataset is **17 Dollars**\n- The max price in the dataset is **2000 Dollars**\n- Most item prices are at about **10 Dollars**"],"metadata":{}},{"cell_type":"markdown","source":["**Why take log(price)?** \n\nGenerally, the Root Mean Squared Error (RMSE) metric is used for regression tasks. But as price followed a long-tailed distribution (50% of the products were under $10), in order to make errors on low price product more relevant than for higher prices, the metric chosen for competition evaluation was Root Mean Squared Logarithmic Error (RMSLE). Thus, I applied the log transformation to the price target variable, to make this assumption available for model training.\n\n\n**Example:**\n\n**Step 1 Log Transformation:** np.log(train['price']+1)\n\n**Step 2 Predict with Log Transformation:** test_pred = model.predict(X_test)\n\n**Step 3 Convert back to original value by Exponential Transformation** Y_test = np.expm1(test_pred)"],"metadata":{}},{"cell_type":"code","source":["# Plot Price Distribution\nplt.subplot(1, 2, 1)\n(train['price']).plot.hist(bins=50, figsize=(15, 6), edgecolor = 'white', range = [0, 250])\nplt.xlabel('price', fontsize=12)\nplt.title('Price Distribution', fontsize=12)\n\n#Plot Log Price Distribution\nplt.subplot(1, 2, 2)\nnp.log(train['price']+1).plot.hist(bins=50, figsize=(15,6), edgecolor='white')\nplt.xlabel('log(price+1)', fontsize=12)\nplt.title('Log Price Distribution', fontsize=12)\n\nplt.show()"],"metadata":{"scrolled":true},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["## Remove Items with 0 Price"],"metadata":{}},{"cell_type":"code","source":["train[train.price==0]"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# We have 311 items with price of $0. Let's take them out because it looks like an error on their part. \ntrain[train.price==0].shape"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# Remove items with price of $0 from our training set\ntrain = train[train.price != 0]\ntrain.shape"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["## Shipping Distribution"],"metadata":{"collapsed":true}},{"cell_type":"code","source":["train['shipping'].value_counts() / len(train)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["## Price Distribution by Shipping Type"],"metadata":{}},{"cell_type":"markdown","source":["Seems about right. Shipping does increase the price value and confirms our intiution."],"metadata":{}},{"cell_type":"code","source":["shipping_fee_by_buyer = train.loc[train['shipping'] == 0, 'price']\nshipping_fee_by_seller = train.loc[train['shipping'] == 1, 'price']\n\nfig, ax = plt.subplots(figsize=(18,8))\n\nax.hist(shipping_fee_by_seller, color='blue', alpha=1.0, bins=50, range = [0, 100],label='Price when Seller pays Shipping')\nax.hist(shipping_fee_by_buyer, color='red', alpha=0.7, bins=50, range = [0, 100],label='Price when Buyer pays Shipping')\n\nplt.xlabel('price', fontsize=12)\nplt.ylabel('frequency', fontsize=12)\nplt.title('Price Distribution by Shipping Type', fontsize=15)\nplt.tick_params(labelsize=12)\nplt.legend()\nplt.show()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["# Average Price for Shipping Type"],"metadata":{}},{"cell_type":"code","source":["print('The average price is {}'.format(round(shipping_fee_by_seller.mean(), 2)), 'if seller pays shipping');\nprint('The average price is {}'.format(round(shipping_fee_by_buyer.mean(), 2)), 'if buyer pays shipping')"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["## Top 10 Categories"],"metadata":{}},{"cell_type":"code","source":["top_10_category = train['category_name'].value_counts()[:10].reset_index()\ntop_10_category"],"metadata":{"scrolled":true},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["# Top 10 Categories Distribution"],"metadata":{}},{"cell_type":"code","source":["top_10_category = train['category_name'].value_counts()[:10].reset_index()\n\nsns.set(style=\"whitegrid\")\n\n# Initialize the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 10))\n\n# Plot the total items per category\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=\"category_name\", y='index', data=top_10_category,\n            label=\"# Items\", color=\"g\")\n\n# Add a legend and informative axis label\nax.set( ylabel=\"Item Category\", title='# Item Per Category',\n       xlabel=\"# of Items\")\n\nplt.show()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["## Create new Feature by Splitting Category\n\nThere's a format in the category feature: **Root Category/Category/Subcategory**. In the given dataset also it is following the same trend so we need to split the category and save each of them in a separate column."],"metadata":{}},{"cell_type":"code","source":["# Feature Engineering (1): Creating new Category Features\ndef transform_category_name(category_name):\n    try:\n        main, sub1, sub2= category_name.split('/')\n        return main, sub1, sub2\n    except:\n        return 'Other','Other','Other'"],"metadata":{"scrolled":true},"outputs":[],"execution_count":44},{"cell_type":"code","source":["train['category_main'], train['category_sub1'], train['category_sub2'] = zip(*train['category_name'].apply(transform_category_name))\n\ncat_train = train[['category_main','category_sub1','category_sub2', 'price']]\n\ncat_train.head()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["# Top 10 Main Category\nWomen and beauty products make up the most of the items. This is interesting because it allows us to know who are users are, which we can then probably do some sort of targeted marketing..."],"metadata":{}},{"cell_type":"code","source":["plt.figure(figsize=(17,10))\nsns.countplot(y = train['category_main'], order = train['category_main'].value_counts().index, orient = 'v')\nplt.title('Top 10 Main Categories', fontsize = 25)\nplt.ylabel('Main Category', fontsize = 20)\nplt.xlabel('Number of Items')\nplt.show()"],"metadata":{"scrolled":true},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["# Ratio of Main Category \nWomen takes up about 45 percent of the main category"],"metadata":{}},{"cell_type":"code","source":["# Look at the ratio of category for items\ntrain['category_main'].value_counts()/len(train)"],"metadata":{"scrolled":true},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["# Brand Analysis"],"metadata":{}},{"cell_type":"markdown","source":["There is about 3750 unique brands"],"metadata":{}},{"cell_type":"code","source":["# Amount of unique brand names\ntrain['brand_name'].nunique()"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["# Top 20 Brand Distribution\nb20 = train['brand_name'].value_counts()[1:20].reset_index().rename(columns={'index': 'brand_name', 'brand_name':'count'})\nax = sns.barplot(x=\"brand_name\", y=\"count\", data=b20)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nax.set_title('Top 20 Brand Distribution', fontsize=15)\nplt.show()"],"metadata":{"scrolled":true},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["## Top 20 Expensive Brands"],"metadata":{}},{"cell_type":"code","source":["top_20_exp_brand = pd.DataFrame(train.groupby(['brand_name'],as_index=True).std().price.sort_values(ascending=False)[0:20]).reset_index()\nax = sns.barplot(x=\"brand_name\", y=\"price\", data=top_20_exp_brand)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nax.set_title('Top 20 Expensive Brand Distribution', fontsize=15)\nplt.show()"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["# Length of Description VS Price"],"metadata":{}},{"cell_type":"markdown","source":["Does the length of description have some affect on pricing?"],"metadata":{}},{"cell_type":"code","source":["train.item_description = train.item_description.astype(str)\n\ndescr = train[['name','item_description', 'price']]\ndescr['count'] = descr['item_description'].apply(lambda x : len(str(x)))\ndescr.head()"],"metadata":{"scrolled":false},"outputs":[],"execution_count":58},{"cell_type":"code","source":["df = descr.groupby('count')['price'].mean().reset_index()\nsns.regplot(x=df[\"count\"], y=(df[\"price\"]))\nplt.xlabel(\"word count\")\nplt.show()"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["# Text Processing\n***\nLet's normalize the words by:\n- Removing Punctuations\n- Removing Stop Words\n- Lowercasing the Words\n- Stemming the Words"],"metadata":{}},{"cell_type":"markdown","source":["### List of Punctuations"],"metadata":{}},{"cell_type":"code","source":["from string import punctuation\npunctuation"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["# Create a list of punctuation replacements\npunctuation_symbols = []\nfor symbol in punctuation:\n    punctuation_symbols.append((symbol, ''))\n    \npunctuation_symbols"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["### List of Stop Words"],"metadata":{}},{"cell_type":"code","source":["# Examine list of stop words\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nstop"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["### Create Functions to Normalize the Words"],"metadata":{}},{"cell_type":"code","source":["import string\n\n# Create a function to remove punctuations\ndef remove_punctuation(sentence: str) -> str:\n    return sentence.translate(str.maketrans('', '', string.punctuation))\n\n# Create a function to remove stop words\ndef remove_stop_words(x):\n    x = ' '.join([i for i in x.lower().split(' ') if i not in stop])\n    return x\n\n# Create a function to lowercase the words\ndef to_lower(x):\n    return x.lower()\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":["### Apply Normalizing Functions"],"metadata":{}},{"cell_type":"code","source":["# Stem the Words\nfrom nltk.stem.porter import PorterStemmer\nporter = PorterStemmer()\ntrain['item_description'] = train['item_description'].apply(porter.stem)\n\ntrain['item_description'] = train['item_description'].apply(remove_punctuation)\ntrain['item_description'] = train['item_description'].apply(remove_stop_words)\ntrain['item_description'] = train['item_description'].apply(to_lower)\n\ntrain['name'] = train['name'].apply(remove_punctuation)\ntrain['name'] = train['name'].apply(remove_stop_words)\ntrain['name'] = train['name'].apply(to_lower)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":["# Feature Extraction with Text\n***"],"metadata":{}},{"cell_type":"markdown","source":["**Feature Pre-Processing:**\nSometimes you can't just fit a dataset into your model and expect good results. Each type of feature has their own way of preprocessing. Choice of preprocessing method also depends on the model we are trying to use.\n\nSince we're working with Text Features, we're going to do a lot of vectorization:\n- Tokenization: split each text into words (bag of words model)\n- Stemming: removing word inflections (getting the root word)\n- Vectorization: reducing text into a vector with different types of frequencies for each word (Count Values or TF-IDF Values)"],"metadata":{}},{"cell_type":"markdown","source":["## Bag of Words\nWhen we vectorize these words, we're doing creating a feature for each word. Also known as, **Bag of Words**. **We lose word ordering**\n\n**Solution**: To preserve some ordering, we can introduce **n-grams** into our vectorization of words (problem: too many features)\n- one way to reduce dimensions of n-grams is to remove stop words (a, the, is)\n- **stop words**: we can remove these words becaues they are just there for grammatical structure with little to no meaning\n- **n-gram** with smaller frequencies can highlight and capture important parts of a document/text. This **preserves local ordering** and **can improve model performance**."],"metadata":{}},{"cell_type":"code","source":["# Examine the normalize item description\ntrain['item_description'][115:125]"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["#import nltk\n#nltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\n\ntext1 = train['item_description'][120]\ntokens = word_tokenize(text1)\nprint(tokens)"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":["## Word Frequency Weights\nEach word in our feature space can have different frequency weights\n- Frequency Weight\n- TF-IDF Weight\n- Binary\n\n\n**CountVectorizer**: Returns an encoded vector with integer count for each word\n\n**TF-IDF(min_df, max_df, n-gram)**: Returns encoded vector with weighted count for each word (utilizes the word in the document in corresponsdence to the whole corpus, to keep it short, more emphasis on the rarity of a word).  This is good because we want to find frequent terms from that document that isn't so frequent within the whole document corpus.\n\n**LabelBinarizer**: Get's all the word and assigns it to its own column. 0 means it's there and 1 means not (example with brand names)\n\n**Why are we doing this again?**: Because some Machine Learning models don't recognize text as well. You're going to have to convert it into numbers"],"metadata":{}},{"cell_type":"markdown","source":["# Feature Engineering\n***"],"metadata":{}},{"cell_type":"markdown","source":["**BONUS** Extra Feature Engineering Ideas\n- Character Count\n- Word Count\n- Number of Unique Words\n- Average Post Length of Main Category\n- If brand yes/no feature\n- Etc.."],"metadata":{}},{"cell_type":"code","source":["# Look at our features\ntrain.columns"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":["### Categorical Variables (Need to do Encoding):\nHow should we encode these features?\n- name\n- brand_name\n- category_main, category_sub1, category_sub2\n- item_description\n- shipping\n- item_condition_id"],"metadata":{}},{"cell_type":"markdown","source":["# CountVectorizer"],"metadata":{}},{"cell_type":"code","source":["# CountVectorizer - name & categories\ncv = CountVectorizer(min_df=10)\nX_name = cv.fit_transform(train['name'])\nX_category_main = cv.fit_transform(train['category_main'])\nX_category_sub1 = cv.fit_transform(train['category_sub1'])\nX_category_sub2 = cv.fit_transform(train['category_sub2'])"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["print(\"Item Name Shape: \" + str(X_name.shape))\nprint(\"Category Main Shape: \" + str(X_category_main.shape))\nprint(\"Category Sub1 Shape: \" + str(X_category_sub1.shape))\nprint(\"Category Sub2 Shape: \" + str(X_category_sub2.shape))"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"markdown","source":["# LabelBinarizer"],"metadata":{}},{"cell_type":"code","source":["# Apply LabelBinarizer to \"brand_name\"\nlb = LabelBinarizer(sparse_output=True)\nX_brand = lb.fit_transform(train['brand_name'])"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["print(\"Item Brand Shape: \" + str(X_brand.shape))"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"markdown","source":["# Get_Dummies"],"metadata":{}},{"cell_type":"code","source":["# Apply get_dummies to \"item_condition_id\" and \"shipping\" and then convert into a CSR Matrix\nX_dummies = csr_matrix(pd.get_dummies(train[['item_condition_id', 'shipping']], sparse=True).values)"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"markdown","source":["# TFIDF\n**Main Goal:** Measure hwo important a word or phrase is within a collection of documents. It essentially **weigh down** terms that appear frequently and **scale up** unique terms.\n\n**TF Term Frequency** how often a term occurs \n\n**IDF Inverse Document Frequency** how important a term is\n\n### Important Parameters\n1. ngram_range \n2. stop_words \n3. lowercase \n4. max_df - max threshold that will ignore a term that has a document frequency higher than the threshold\n5. min_df - min threshold that will ignore a term that has a document frequency lower than the threshold\n6. max_features - gets the assigned amount of features with highest amount of term frequencies (scores)"],"metadata":{}},{"cell_type":"code","source":["# Perform a TFIDF Transformation of the item description with the top 55000 features and has an n-gram range of 1-2\ntv = TfidfVectorizer(max_features=55000, ngram_range=(1, 2), stop_words='english')\nX_description = tv.fit_transform(train['item_description'])"],"metadata":{"collapsed":true},"outputs":[],"execution_count":89},{"cell_type":"code","source":["print(\"Item Description Shape: \" + str(X_description.shape))"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"markdown","source":["### Observing the TFIDF Weights"],"metadata":{}},{"cell_type":"code","source":["#  create a dictionary mapping the tokens to their tfidf values\ntfidf = dict(zip(tv.get_feature_names(), tv.idf_))\ntfidf = pd.DataFrame(columns=['tfidf']).from_dict(\n                    dict(tfidf), orient='index')\ntfidf.columns = ['tfidf']\n\n# Lowest TFIDF Scores\ntfidf.sort_values(by=['tfidf'], ascending=True).head(10)"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["# HIghest TFIDF Scores\ntfidf.sort_values(by=['tfidf'], ascending=False).head(10)"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"markdown","source":["# Combine All Features Into One Merge"],"metadata":{}},{"cell_type":"code","source":["# Combine everything together\nsparse_merge = hstack((X_dummies, X_description, X_brand, X_name, X_category_main, X_category_sub1, X_category_sub2)).tocsr()"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"markdown","source":["### To be continued..."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{"collapsed":true},"outputs":[],"execution_count":97}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.4","nbconvert_exporter":"python","file_extension":".py"},"name":"Mercari-Challenge-Part-1","notebookId":3984361640103792},"nbformat":4,"nbformat_minor":0}
